{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 1 \n",
    "Course 2023/24: F. Chiariotti\n",
    "\n",
    "The notebook contains some simple tasks to be performed about **classification and regression**. <br>\n",
    "Complete all the **required code sections** and **answer to all the questions**. <br>\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "1. **Read carefully all cells** and **follow the instructions**\n",
    "1. **Rerun all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "2. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "3. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command** - otherwise points will be deduced.\n",
    "4. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB1.ipynb** otherwise your homework could get lost\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Andrea Turci<br>\n",
    "**ID Number**: 2106724"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. \n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features.\n",
    "\n",
    "<center>\n",
    "\n",
    "![COVER](data/dataset-cover.png \"COVER\")\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first **import** all **the packages** that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change some global settings for layout purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are in the jupyter notebook environment you can change the 'inline' option with 'notebook' to get interactive plots\n",
    "%matplotlib inline\n",
    "# change the limit on the line length and crop to 0 very small numbers, for clearer printing\n",
    "np.set_printoptions(linewidth=500, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1) Perceptron\n",
    "In the following cells we will **implement** the **perceptron** algorithm and use it to learn a halfspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.0):** **Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: 1234567 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDnumber = 2106724 # YOUR_ID\n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset and split it** in training and test set (the **training** set is **typically larger**, here we use a 75% training 25% test split).\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above. Try different seeds to evaluate the impact of randomization.<br><br>\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')\n",
    "#print(X,Y)\n",
    "#print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training and test sets\n",
    "\n",
    "\n",
    "\n",
    "Given $m$ total data, denote with $m_{t}$ the part used for training. Keep $m_t$ data as training data, and $m_{test}:= m-m_{t}$. <br>\n",
    "For instance one can take $m_t=0.75m$ of the data as training and $m_{test}=0.25m$ as testing. <br>\n",
    "Let us define as define\n",
    "\n",
    "$\\bullet$ $S_{t}$ the training data set\n",
    "\n",
    "$\\bullet$ $S_{test}$ the testing data set\n",
    "\n",
    "\n",
    "The reason for this splitting is as follows:\n",
    "\n",
    "TRAINING DATA: The training data are used to compute the empirical loss\n",
    "$$\n",
    "L_S(h) = \\frac{1}{m_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "which is used to estimate $h$ in a given model class ${\\cal H}$.\n",
    "i.e. \n",
    "$$\n",
    "\\hat{h} = {\\rm arg\\; min}_{h \\in {\\cal H}} \\, L_S(h)\n",
    "$$\n",
    "\n",
    "TESTING DATA: The test data set can be used to estimate the performance of the final estimated model\n",
    "$\\hat h_{\\hat d_j}$ using:\n",
    "$$\n",
    "L_{{\\cal D}}(\\hat h_{\\hat d_j}) \\simeq \\frac{1}{m_{test}} \\sum_{ z_i \\in S_{test}} \\ell(\\hat h_{\\hat d_j},z_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   44.45 63.  ]\n",
      " [54.   20.05 58.  ]\n",
      " [ 9.   19.6  53.  ]\n",
      " ...\n",
      " [ 7.   19.5  48.  ]\n",
      " [49.   95.6  53.  ]\n",
      " [26.   89.8  49.  ]]\n",
      "[ 1 -1 -1 ... -1  1  1]\n",
      "Number of samples in the train set: 2817\n",
      "Number of samples in the test set: 940\n",
      "\n",
      "Number of night instances in test: 467\n",
      "Number of day instances in test: 473\n",
      "Mean of the training input data: [ 0. -0.  0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [0.05656088 0.05900971 0.00638751]\n",
      "Std of the test input data: [1.0164534  0.98155071 1.00688402]\n"
     ]
    }
   ],
   "source": [
    "# compute the splits\n",
    "m_training = int(0.75 * len(Y))\n",
    "#print(len(Y))\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  len(Y) - m_training\n",
    "\n",
    "# X_training = instances for training set\n",
    "X_training= X[:m_training]       \n",
    "print(X_training)               \n",
    "#print(len(X_training))\n",
    "\n",
    "# Y_training = labels for the training set\n",
    "Y_training = Y[:m_training]     \n",
    "print(Y_training)                   \n",
    "#print(len(Y_training))\n",
    "\n",
    "# X_test = instances for test set\n",
    "X_test = X[m_training:]                    \n",
    "#print(len(X_test))\n",
    "\n",
    "# Y_test = labels for the test set\n",
    "Y_test = Y[m_training:]                       \n",
    "#print(len(Y_test))\n",
    "\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"\\nNumber of night instances in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of day instances in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# standardize the input matrix\n",
    "# the transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.1):** **Divide** the **data into training and test set** (**75%** of the data in the **first** set, **25%** in the **second** one). <br>\n",
    "<br>\n",
    "Notice that as is common practice in Statistics and Machine Learning, **we scale the data** (= each variable) so that it is centered **(zero mean)** and has **standard deviation equal to 1**. <br>\n",
    "This helps in terms of numerical conditioning of the (inverse) problems of estimating the model (the coefficients of the linear regression in this case), as well as to give the same scale to all the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **add a 1 in front of each sample** so that we can use a vector in **homogeneous coordinates** to describe all the coefficients of the model. This can be done with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    # Add a 1 to each sample (homogeneous coordinates)\n",
    "    X_training = np.hstack( [np.ones( (X_training.shape[0], 1) ), X_training] )\n",
    "    X_test = np.hstack( [np.ones( (X_test.shape[0], 1) ), X_test] )\n",
    "    \n",
    "    return X_training, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set in homogeneous coordinates:\n",
      "[[ 1.         -1.17988221 -0.74922977  0.9108261 ]\n",
      " [ 1.          1.0426461  -1.55981591  0.61697422]\n",
      " [ 1.         -0.84440624 -1.57476524  0.32312233]\n",
      " [ 1.         -1.17988221 -1.57642628 -1.20490748]\n",
      " [ 1.         -0.97020973  0.88190873 -0.32335182]\n",
      " [ 1.         -0.59279926  0.53807403  0.73451497]\n",
      " [ 1.          0.41362865  0.49488707 -0.02949993]\n",
      " [ 1.          0.83297362  1.36028743 -0.55843333]\n",
      " [ 1.         -0.59279926 -0.39875093  1.49852988]\n",
      " [ 1.          0.70717013 -0.88377378  0.9108261 ]]\n"
     ]
    }
   ],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.2):** Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout .\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    # This function:\n",
    "    # computes the number of misclassified samples\n",
    "    # returns the index of all misclassified samples\n",
    "    # if there are no misclassified samples, returns -1 as index\n",
    "  \n",
    "    # ADD YOUR CODE HERE\n",
    "    # WRITE THE FUNCTION\n",
    "    sum1 = 0\n",
    "    index = [] \n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if Y[i]*np.dot(X[i],current_w) <=0:                           \n",
    "            index.append(i)\n",
    "            sum1 += 1\n",
    "    \n",
    "    if index != []: \n",
    "        return sum1, index\n",
    "    else:\n",
    "        return sum1, -1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def perceptron_update(current_w, x, y):\n",
    "    # Place in this function the update rule of the perceptron algorithm\n",
    "    # Remember that numpy arrays can be treated as generalized variables\n",
    "    # therefore given array a = [1,2,3,4], the operation b = 10*a will yield\n",
    "    # b = [10, 20, 30, 40]\n",
    "    \n",
    "    new_w = current_w + y*x\n",
    "\n",
    "    return new_w\n",
    "\n",
    "\n",
    "def perceptron_no_randomization(X, Y, max_num_iterations):\n",
    "    \n",
    "    # Initialize some support variables\n",
    "    num_samples = X.shape[0]    #è il numero di righe dell'array\n",
    "    \n",
    "    # best_errors will keep track of the best (minimum) number of errors\n",
    "    # seen throughout training, used for the update of the best_w variable\n",
    "    best_error = num_samples+1\n",
    "    \n",
    "    # Initialize the weights of the algorith with w=0\n",
    "    curr_w = np.zeros(4) \n",
    "    # The best_w variable will be used to keep track of the best solution\n",
    "    best_w = curr_w.copy()\n",
    "\n",
    "    # compute the number of misclassified samples and the index of the first of them\n",
    "    num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "    # update the 'best' variables\n",
    "    if num_misclassified < best_error:\n",
    "        best_error = num_misclassified    \n",
    "        best_w = curr_w\n",
    "    \n",
    "    # initialize the number of iterations\n",
    "    num_iter = 0\n",
    "    # Main loop continue until all samples correctly classified or max # iterations reached\n",
    "    # Remember that to signify that no errors were found we set index_misclassified = -1\n",
    "    while index_misclassified != -1 and num_iter < max_num_iterations:\n",
    "        # ADD YOUR CODE HERE\n",
    "        # COMPLETE THE WHILE LOOP\n",
    "        # Choose the misclassified sample with the lowest index at each iteration\n",
    "            \n",
    "        curr_w = perceptron_update(best_w, X[index_misclassified[0]], Y[index_misclassified[0]])          \n",
    "\n",
    "        num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "\n",
    "        if num_misclassified < best_error:\n",
    "            best_error = num_misclassified\n",
    "            best_w = curr_w\n",
    "\n",
    "\n",
    "        num_iter += 1\n",
    "\n",
    "    # as required, return the best error as a ratio with respect to the total number of samples\n",
    "    best_error = best_error/num_samples\n",
    "    \n",
    "    return best_w, best_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (30 iterations): 0.2641107561235357\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 100 iterations\n",
    "w_found, error = perceptron_no_randomization(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "#print(\"Training Weight of perceptron (30 iterations): \" + str(w_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.3):** use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of perceptron (30 iterations): 0.25\n"
     ]
    }
   ],
   "source": [
    "errors, _ = count_errors(w_found, X_test,Y_test)\n",
    "\n",
    "true_loss_estimate = errors/len(X_test)     # Error rate on the test set\n",
    "# Note: you can avoid using num_errors if you prefer, as long as true_loss_estimate is correct\n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.4):** implement the correct randomized version of the perceptron such that at each iteration the algorithm picks a random misclassified sample and updates the weights using that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X, Y, max_num_iterations):    \n",
    "    num_samples = X.shape[0] \n",
    "    \n",
    "    best_error = num_samples+1\n",
    "    \n",
    "    curr_w = np.random.rand(4)\n",
    "    best_w = curr_w.copy()\n",
    "\n",
    "    num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "    if num_misclassified < best_error:\n",
    "        best_error = num_misclassified \n",
    "        best_w = curr_w\n",
    "    \n",
    "    num_iter = 0\n",
    "    while index_misclassified != -1 and num_iter < max_num_iterations:\n",
    "        \n",
    "        ran_value =np.random.rand(4)\n",
    "\n",
    "        curr_w = perceptron_update(best_w, ran_value, Y[np.random.randint(0, len(index_misclassified))])\n",
    "\n",
    "        num_misclassified, index_misclassified = count_errors(curr_w, X, Y)\n",
    "\n",
    "        if num_misclassified < best_error:\n",
    "            best_error = num_misclassified\n",
    "            best_w = curr_w\n",
    "\n",
    "        num_iter += 1\n",
    "        \n",
    "    best_error = best_error/num_samples\n",
    "    \n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.5):** Now test the correct version of the perceptron using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (30 iterations): 0.26198083067092653\n",
      "Test Error of perceptron (30 iterations): 0.2712765957446808\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 30 iterations\n",
    "w_found, error = perceptron(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "#print(\"Training Weight of perceptron (30 iterations): \" + str(w_found))\n",
    "\n",
    "errors, _ = count_errors(w_found, X_test,Y_test)\n",
    "\n",
    "true_loss_estimate = errors/len(X_test)     # Error rate on the test set\n",
    "# Note: you can avoid using num_errors if you prefer, as long as true_loss_estimate is correct\n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q2) [Answer the following]** <br>\n",
    "What is the difference between the two versions of the perceptron? Can you explain why there is this difference? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q2**:<br>\n",
    "The main difference between the two implementations of the perceptrons is the way the weights are updated after each round of predictions, during training. In the first case, the misclassified sample with the lowest index is chosen at each iteration for updating the weights (thus using a deterministic approach). The loop continues until all samples are correctly classified or the maximum number of iterations is reached. Otherwise, in the second case, a random misclassified sample is chosen at each (introducing randomness), and, like before, the loop also continues until either correct classification or reaching the maximum number of iterations. This last version makes use of a stochastic approach which can help in escaping local minima during training and may lead to faster convergence, especially in situations where the data is not linearly separable.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now consider only a the random version of the perceptron\n",
    "\n",
    "**TO DO (A.Q2) [Answer the following]** <br>\n",
    "What about the difference between the training error and the test error  in terms of fraction of misclassified samples? Explain what you observe. (Notice that with a very small dataset like this one results can change due to randomization, try to run with different random seeds if you get unexpected results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q2**:<br>\n",
    "Because of the size of the dataset, randomization is crucial to these outcomes, as the question clearly states. Using different seeds yields varying outcomes. For example, some seeds create a test set that is higher in error than the training set, while others produce results that are nearly equal for both the training and test sets. Some seeds also produce a test set that is lower in error than the training set. In any case, the values are consistently quite similar, which leads us to believe that the test set is an accurate representation of the training set, and that learning the latter creates a model that anticipates other data (particularly, the data in the training set) with a comparable level of accuracy. The results show that the lowest mistakes were around 25%.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss with respect to the number of iterations\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "num_iters = np.arange(0, 1001, 20)\n",
    "errors = []\n",
    "\n",
    "for num_iter in num_iters:\n",
    "    _, error = perceptron(X_training, Y_training, num_iter)\n",
    "    errors.append(error)\n",
    "\n",
    "plt.plot(num_iters, errors)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Training error')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Note how the training loss decreases as we increase the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.1.6):** Copy the code from the last 2 cells above in the cell below and repeat the training with 3000 iterations. Then print the error in the training set and the estimate of the true loss obtained from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error of perceptron (3000 iterations): 0.24813631522896698\n",
      "Test Error of perceptron (3000 iterations): 0.24148936170212765\n"
     ]
    }
   ],
   "source": [
    "w_found, error = perceptron(X_training,Y_training, 3000)\n",
    "print(\"Training Error of perceptron (3000 iterations): \" + str(error))\n",
    "\n",
    "errors, _ =  count_errors(w_found, X_test,Y_test) \n",
    "\n",
    "true_loss_estimate = errors/len(X_test)\n",
    "print(\"Test Error of perceptron (3000 iterations): \" + str(true_loss_estimate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q3) [Answer the following]** <br>\n",
    "What about the difference between the training error and the test error in terms of the fraction of misclassified samples) when running for a larger number of iterations? Explain what you observe and compare with the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q3**:<br>\n",
    "The fraction of misclassified samples decreases using a larger number of iterations, even though the ratios between the training error and the test error are quite similar to each other and compatibles. This means that increasing the number of the iteration makes the weight more precise, as expected.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.2) Logistic Regression\n",
    "Now we use **logistic regression**, exploiting the implementation in **Scikit-learn**, to predict labels. We will also plot the decision boundaries of logistic regression.\n",
    "\n",
    "We first load the dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a logistic regression model in Scikit-learn use the instruction\n",
    "\n",
    "$linear\\_model.LogisticRegression(C=1e5)$\n",
    "\n",
    "($C$ is a parameter related to *regularization*, a technique that\n",
    "we will see later in the course. Setting it to a high value is almost\n",
    "as ignoring regularization, so the instruction above corresponds to the\n",
    "logistic regression you have seen in class.)\n",
    "\n",
    "To learn the model you need to use the $fit(...)$ instruction and to predict you need to use the $predict(...)$ function. <br>\n",
    "See the Scikit-learn documentation for how to use it [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "**TO DO (A.2.2):** **Define** the **logistic regression** model, then **learn** the model using **the training set** and **predict** on the **test set**. Then **print** the **fraction of samples misclassified** in the training set and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-0.04773262]\n",
      "Coefficients: [[-0.04773262 -1.4131979   0.83754426  0.2353059 ]]\n",
      "Error rate on training set: 0.24813631522896698\n",
      "Error rate on test set: 0.23617021276595745\n"
     ]
    }
   ],
   "source": [
    "# part on logistic regression for 2 classes\n",
    "logreg = linear_model.LogisticRegression(C=1e5) # C should be very large to ignore regularization (see above)\n",
    "\n",
    "# learn from training set: hint use fit(...)\n",
    "# ADD YOUR CODE HERE\n",
    "logreg.fit(X_training, Y_training)\n",
    "print(\"Intercept:\" , logreg.intercept_)\n",
    "print(\"Coefficients:\" , logreg.coef_)\n",
    "\n",
    "# predict on training set\n",
    "predicted_training = logreg.predict(X_training)\n",
    "\n",
    "# print the error rate = fraction of misclassified samples\n",
    "error_count_training = (predicted_training != Y_training).sum()\n",
    "error_rate_training = error_count_training/len(predicted_training)\n",
    "print(\"Error rate on training set: \"+str(error_rate_training))\n",
    "\n",
    "# predict on test set\n",
    "predicted_test = logreg.predict(X_test)\n",
    "\n",
    "#print the error rate = fraction of misclassified samples\n",
    "error_count_test = (predicted_test != Y_test).sum()\n",
    "error_rate_test = error_count_test/len(predicted_test)\n",
    "print(\"Error rate on test set: \" + str(error_rate_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.2.3)** Now **pick two features** and restrict the dataset to include only two features, whose indices are specified in the $idx0$ and $idx1$ variables below. Then split into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names  = [\"Tenure in Months\",\"Monthly Charge\",\"Age\"]\n",
    "\n",
    "# Select the two features to use\n",
    "idx0 = 0\n",
    "idx1 = 1\n",
    "idx2 = 2\n",
    "\n",
    "X_reduced = X[:, [idx0, idx1]]\n",
    "\n",
    "#print(X_reduced)\n",
    "\n",
    "X_training= X_reduced[:m_training]                           \n",
    "Y_training = Y_training                                   \n",
    "\n",
    "X_test = X_reduced[m_training:]                            \n",
    "Y_test = Y_test                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now learn a model using the training data and measure the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-0.30408025]\n",
      "Coefficients: [[-0.0592678  0.0288422]]\n",
      "Error rate on test set: 0.2393617021276596\n"
     ]
    }
   ],
   "source": [
    "# learning from training data\n",
    "logreg.fit(X_training, Y_training)\n",
    "print(\"Intercept:\" , logreg.intercept_)\n",
    "print(\"Coefficients:\" , logreg.coef_)\n",
    "# predict on test set\n",
    "predicted_test = logreg.predict(X_test)\n",
    "\n",
    "#print the error rate = fraction of misclassified samples\n",
    "error_count_test = (predicted_test != Y_test).sum()\n",
    "\n",
    "# print the error rate = fraction of misclassified samples\n",
    "error_rate_test = error_count_test/len(predicted_test)\n",
    "print(\"Error rate on test set: \" + str(error_rate_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO (A.Q4) [Answer the following]** <br>\n",
    "Which features did you select and why? <br>\n",
    "Compare the perfomance of the classifiers trained with every combination of two features with that of the baseline (which used all 3 features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q4**:<br>\n",
    "I selected 'Tenure in Months'-'Monthly Charge' as feature, since Age is the least significant feature. Indeed, as we can see, trying with all combinations of the two features I get (with this run): <br>\n",
    "'Tenure in Months'-'Age'<br>\n",
    "Error rate on test set: 0.3021276595744681\n",
    "\n",
    "'Monthly Charge'-'Age'  <br>\n",
    "Error rate on test set: 0.39148936170212767 <br>\n",
    "\n",
    "'Tenure in Months'-'Monthly Charge' <br>\n",
    "Error rate on test set: 0.2542553191489362 <br>\n",
    "\n",
    "While using all 3 features: <br>\n",
    "Error rate on test set: 0.2542553191489362 <br>\n",
    "\n",
    "As depicted, the error rate with all 3 features is the same as the one removing the feature Age.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
